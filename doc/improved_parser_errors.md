Improved parsing with position error messages using Alex and Happy in Haskell
=============================================================================

In this post I'll discuss improving our `happy`-based parser with better error
messages, in anticipation of them being necessary for debugging later stages of
the Tiger compiler.

# Negative Unit Tests and Error Messages

In the last few posts, I've implemented a lexer and parser for the Tiger
language by using `alex` and `happy`. Starting from more basic implementations
typically covered in tutorials, we expanded the parsing phase using more
advanced features to keep track of things like nested comments and strings that
are not typically covered in tutorials (despite being so important to many
programming languages!). We also implemented a unit testing harness using the
`Tasty` framework.

One issue I overlooked when it comes to testing is testing for negative results.
What I mean is, while we want to confirm obviously that our parser accepts and
correctly parses correct Tiger programs, we also want some assurance that our
parser will *not* accept malformed Tiger programs. It is important for our unit
testing to include a few test cases that fail on purpose, to ensure that our
parser is rejecting things that it should reject, and for the correct reason.

Currently, our lexer and parser only provide a generic "Error encountered" sort
of message. There is no metadata to tell us what kind of error was encountered,
so this message is completely unhelpful. We should look to expand our error
messages with metadata to aid users of our compiler in locating errors in their
code.

# Input Stream Position

The first improvement we can make, which in some
sense is also a requirement for further improvements, is to track our position
within the input stream. We want to track how many characters (or bytes in
general) of input we have read in when we encounter the error; this gives us the
idea of which token of input we failed at.

## Position in `alex`

`alex` actually  provides a wrapper called "posn" that provides built-in
functions to track the position within the input stream. If you're using the
"basic" wrapper, you can  switch to "posn" to get this information with almost
no effort.

However, in the last post, I decided to use `alex`'s low-level API in order to
implement lexer states so comments and string literals could be tracked easily.
This means we can't use the "posn" wrapper without re-writing our code fully,
and as we previously determined, those wrappers weren't sufficient for correctly
lexing nested comments.

Instead, we will add our own data structure to keep track of the position
information manually.

First, we define an `AlexPosn` record which is similar to the one that is
auto-generated by alex when using the "posn" wrapper (note that I used the same
`AlexPosn` name to keep it easier in my head, but there is no requirement that we
do so).

    data AlexPosn = AlexPosn { absolute :: Int, row :: Int, col :: Int }
      deriving (Eq, Show)

An `AlexPosn` consists of three values: the absolute offset in the stream from the
beginning of the file, and the row and column number in the offset (assuming
that rows are separated by newline characters). I added this definition to the
`Tokens.hs` file so it could be imported into the parser and other modules that
need to access this information.

We add a position field of type `AlexPosn` to the `AlexInput` record:

    data AlexInput = AlexInput {
                    aiprev::Char,
                    aibytes::[Word8],
                    airest::String,
                    aipos::AlexPosn }
                    deriving Show

This allows us to track the position in the input stream as we read characters.
We then add some code to `alexGetByte` to define how we count new rows and
columns:

    alexGetByte :: AlexInput -> Maybe (Word8, AlexInput)
    alexGetByte ai
    = case (aibytes ai) of
        (b:bs) -> Just (b, ai{aibytes=bs})
        [] -> case (airest ai) of
        [] -> Nothing
        (c:cs) -> let m = row $ aipos ai
                        n = col $ aipos ai
                        a = absolute $ aipos ai
                        newpos = if ((aiprev ai)=='\n')
                                then AlexPosn {absolute=a+1,row=m+1,col=1}
                                else AlexPosn {absolute=a+1,row=m,col=n+1}
                        (b:bs) = encode [c] in
                                Just (b, AlexInput {aiprev=c,
                                                    aibytes=bs,
                                                    airest=cs,
                                                    aipos=newpos})

As can be seen, the absolute value always increments, whereas the row will only
increment when a newline character is seen. The column (col) increments always
unless a newline is seen, then it is reset back to 1. Pretty straightforward.

Now that we track position, we need to embed this information in our tokens
somehow. The best answer seems to make a special lexer monad "wrapper" type that
holds the token data along with the position data together. We call this type `L`,
and define it simply as:

    data L a = L { getPos :: AlexPosn, unPos :: a } deriving (Eq, Show)

Therefore we'll more specifically be creating `L Token` type objects, though this
definition is more general.

Lastly, we only need to lightly modify the `readToken` function to return `L Token`
types rather than plain `Token`, and insert the current position info into the
`L Token`. The main change is when we detect a new `Token`:

    AlexToken inp' n act -> do
        let (AlexInput{airest=buf, aipos=pos}) = input s
        put s{input = inp'}
        res <- act n (take n buf)
        case res of
            Nothing -> readToken
            Just t -> return L {getPos=pos, unPos=t}

As you can see, we call input to get the next character, and use pattern
matching in the `let` statement to save the rest of the input as `buf` and the
current position as `pos`. We put the input into our `LexerState` and call our lexer
action on the characters left in the buffer. If we don't get a `Token` we keep
reading, but if we do, we return an `L` that stores the previously saved `pos`
position info along with the `Token t`.

We can do a similar trick to embed the position info for the end-of-file (EOF)
token:

    AlexEOF -> do
        let (AlexInput{aipos=pos}) = input s
        return L {getPos=pos, unPos=TEOF}

This all leads up to our ability to display an error message with position info
when we reach a lexing error.

    AlexError inp' -> error $ "Lexical error at position "
        ++ (show $ aipos inp') ++ " : <<" ++ (show $ airest inp') ++ ">>"

This is the simplest case, but sufficient to print out the current position as
well as the rest of the input buffer to assist the user with figuring out where
and why it didn't lex. A fancier implementation might use `Either` or some other
monadic type to record the error and give more specific error messages, but this
is likely unnecessary at the lexer phase.

## Position in `happy`

Now that our lexer returns `L Token` items that embed position info, we must
update `happy` to also make use of this information where parsing the data and
displaying error messages.

The token rules get updated to look more like:

    ARRAY       { L { getPos=_, unPos=ARRAY } }

This took me a bit to understand, but the rules are basically pattern matchings,
so we're saying here to associate an `ARRAY` token in the parser with a `L Token`
that contains `ARRAY` as its `unPos` record field value. The value of `getPos` is not
important for matching, so we use an `_` to ignore it. All of the rules get
updated like this.

Slightly more complex are the literal values, like strings and integers. Here's
what they look like:

    NUM         { L { getPos=_, unPos=NUM n } }
    ID          { L { getPos=_, unPos=ID s } }
    STR         { L { getPos=_, unPos=STR s } }

Again, we're pattern matching, so this time we're looking for tokens that
contain fields. `NUM n` matches a `NUM` token followed by something we label as the
identifier `n`. The actual value of `n` is not important to identify that it is a
`Token`, so we can ignore it. In principle we can define an action here that would
use that value for further processing of the token, but there's no need here.

Now, we look at the grammar rules and the abstract syntax tree (AST). We need to
update the rules for literal values to pull the actual literal value out of the
`L Token` first before creating our AST node, so the rules look like:

    NUM         { IntExp (getNum $ unPos $1) }

The `unPos` function was automatically generated from the record definition, and
extracts the `Token` from an `L Token`, but I defined a new `getNum` function too to
make it easy to extra the actual integer value from the `Token`. We do similar for
`STR` and `ID` since they are literal values. Actually, more specifically, they are
terminal values, and so we need to extract the value from the `L Token` directly.

For non-terminal symbols, we have to handle it a little differently.
Non-terminal grammar rules will call an action that forms an AST node from the
results of the actions of components. Since those components are themselves AST
nodes, and not `L Token` directly, we don't need to extract anything and can
simply use the `$1` placeholders as before:

    exp '+' exp     { OpExp { left=$1, oper=Add, right=$3, opposn=(getPos $2) } }

The main difference if we have an extra field to track the position of this
expression in the input stream. I'm inclined to mark the first token of the
expression the position, but I realized it is difficult here since the first
expression `$1` is an AST node and not an `L Token`, so it's less clear to me how to
get the position from it. As a compromise, I decided to set the position based
on the terminal operator token (in this case, the plus sign '+') since it is an
`L Token` and we can get the position of it from the `getPos` function. I don't
think it even worked badly, since it is identifying the operation of the
expression specifically and not just the start of any expression, it's more
easily identified. However, there might be a better way to do it in the future
with some restructuring and better actions.

With these changes, the parser now supports tracking positions of grammar rules
within the AST. I modified the error function to use this info when it cannot
successfully parse a token stream:

    parseError :: [L Token] -> a
    parseError tokens = error ("Parse error: (Line " ++ show errRow ++ ", Col " ++ show errCol ++ "): " ++ show nextTok)
        where errRow = row $ getPos next
            errCol = col $ getPos next
            nextTok = unPos next
            next = head tokens

# Finishing Thoughts

An interesting read and video about compiler error
messages in Haskell can be found [here][1]. I can see how parser generators (or at
least `happy`) are not well equipped to log errors, since they only call actions
when the token string matches a grammar rule; I found [this blog post][2] on not
using parser generators to be interesting as far as error goes.

You can check out the full implementation in my Tiger compiler on GitLab. I
tagged "0.2" to keep track of this state of the compiler as I continue
development. Up Next Now that we have lexed and parsed our original input into
an AST representing a Tiger program, it's time to start looking toward
semantically analyzing the AST to make sure all the *i*'s are dotted and *t*'s are
crossed and that the program makes sense before we try to run it. In particular,
we'll look at how to do some simple typechecking in Tiger. I'm excited!

[1]: https://github.com/jaspervdj/talks/blob/master/2017-skillsmatter-errors/slides.md
[2]: https://mortoray.com/2012/07/20/why-i-dont-use-a-parser-generator/
